#!/usr/bin/env python3
"""
Scene Graph Dataset Processor
å°†åŸå§‹åœºæ™¯æ•°æ®è½¬æ¢ä¸ºHuggingFaceæ•°æ®é›†æ ¼å¼
"""

import json
import os
import glob
from PIL import Image, ImageDraw, ImageFont
from datasets import Dataset, DatasetDict
import random
from tqdm import tqdm
from collections import defaultdict

class SceneGraphProcessor:
    def __init__(self):
        # Functional relationshipæ ‡å‡†åŒ–æ˜ å°„
        self.func_rel_mapping = {
            "open or close": "openorclose",
            "openorclose": "openorclose",
            "adjust": "adjust", 
            "control": "control",
            "provide power": "providepower",
            "providepower": "providepower",
            "activate": "activate"
        }
    
    def find_all_scenes(self, base_folders):
        """é€’å½’æŸ¥æ‰¾æ‰€æœ‰åœºæ™¯æ–‡ä»¶å¤¹"""
        all_scenes = []
        
        for base_folder in base_folders:
            if not os.path.exists(base_folder):
                print(f"âš ï¸ Folder not found: {base_folder}")
                continue
                
            if base_folder.endswith('real'):
                # Realæ•°æ®: /code/real/multiview_subgraphs/1bathroom/uuid/
                pattern = os.path.join(base_folder, "multiview_subgraphs", "*", "*")
                scenes = glob.glob(pattern)
            else:
                # Simæ•°æ®: /code/sim_kitchen/FloorPlan1/2/
                scenes = []
                for root, dirs, files in os.walk(base_folder):
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«JSONå’Œrgbæ–‡ä»¶å¤¹
                    has_json = any(f.endswith('.json') for f in files)
                    has_rgb = 'rgb' in dirs
                    if has_json and has_rgb:
                        scenes.append(root)
            
            all_scenes.extend(scenes)
            print(f"ğŸ“ Found {len(scenes)} scenes in {base_folder}")
        
        print(f"ğŸ¯ Total scenes found: {len(all_scenes)}")
        return all_scenes
    
    def load_scene_data(self, scene_folder):
        """åŠ è½½å•ä¸ªåœºæ™¯çš„JSONå’Œå›¾åƒ"""
        # æ‰¾JSONæ–‡ä»¶
        json_files = glob.glob(os.path.join(scene_folder, "*.json"))
        if not json_files:
            raise FileNotFoundError(f"No JSON file in {scene_folder}")
        
        json_file = json_files[0]  # å–ç¬¬ä¸€ä¸ª
        
        # è¯»å–JSON
        with open(json_file, 'r', encoding='utf-8') as f:
            scene_data = json.load(f)
        
        # æ‰¾RGBå›¾åƒ
        rgb_folder = os.path.join(scene_folder, 'rgb')
        if not os.path.exists(rgb_folder):
            raise FileNotFoundError(f"No rgb folder in {scene_folder}")
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = glob.glob(os.path.join(rgb_folder, "*.jpg")) + \
                     glob.glob(os.path.join(rgb_folder, "*.png"))
        
        if not image_files:
            raise FileNotFoundError(f"No images in {rgb_folder}")
        
        return scene_data, sorted(image_files)
    
    def create_label_mapping(self, nodes):
        """åˆ›å»ºlabelåˆ°ç¼–å·åç§°çš„æ˜ å°„"""
        label_counts = defaultdict(int)
        label_mapping = {}
        
        for node in nodes:
            label = node.get("label", "unknown")
            label_counts[label] += 1
        
        # ä¸ºæ¯ä¸ªnodeåˆ†é…ç¼–å·åç§°
        current_counts = defaultdict(int)
        for node in nodes:
            label = node.get("label", "unknown")
            node_id = node.get("id", "")
            
            if label_counts[label] > 1:
                current_counts[label] += 1
                mapped_name = f"{label}{current_counts[label]}"
            else:
                mapped_name = label
            
            label_mapping[node_id] = mapped_name
        
        return label_mapping
    
    def simplify_graph_structure(self, scene_data):
        """å°†å¤æ‚çš„å›¾ç»“æ„è½¬æ¢ä¸ºç®€åŒ–æ ¼å¼"""
        # è·å–åŸºæœ¬ä¿¡æ¯
        task_instruction = scene_data.get("task_instruction", "")
        action_type = scene_data.get("action_type", "")
        function_type = scene_data.get("function_type", "")
        
        nodes = scene_data.get("nodes", [])
        edges = scene_data.get("edges", [])
        
        # åˆ›å»ºlabelæ˜ å°„
        label_mapping = self.create_label_mapping(nodes)
        
        # ç®€åŒ–nodes
        simplified_nodes = list(label_mapping.values())
        
        # ç®€åŒ–edges
        simplified_edges = []
        for edge in edges:
            obj1_id = edge.get("object1", {}).get("id", "")
            obj2_id = edge.get("object2", {}).get("id", "")
            
            obj1_name = label_mapping.get(obj1_id, "unknown")
            obj2_name = label_mapping.get(obj2_id, "unknown")
            
            # æ ‡å‡†åŒ–functional relationship
            func_rel = edge.get("functional_relationship", "")
            standardized_func_rel = self.func_rel_mapping.get(func_rel, func_rel)
            
            simplified_edge = {
                "functional_relationship": standardized_func_rel,
                "object1": obj1_name,
                "object2": obj2_name,
                "spatial_relations": edge.get("spatial_relations", []),
                "is_touching": edge.get("is_touching", False)
            }
            simplified_edges.append(simplified_edge)
        
        # æ„å»ºç®€åŒ–çš„å›¾ç»“æ„
        simplified_graph = {
            "task_instruction": task_instruction,
            "nodes": simplified_nodes,
            "edges": simplified_edges,
            "action_type": action_type,
            "function_type": function_type
        }
        
        return simplified_graph
    
    def concatenate_images(self, image_files, max_width=2048):
        """æ‹¼æ¥å¤šä¸ªè§†è§’å›¾åƒä¸ºå•å¼ å›¾åƒ (å¤ç”¨benchmarkä»£ç )"""
        if not image_files:
            return None
        
        # è¯»å–å›¾ç‰‡
        images = []
        for img_path in image_files:
            img = Image.open(img_path)
            images.append(img)
        
        n_images = len(images)
        
        # è®¡ç®—ç½‘æ ¼å¸ƒå±€
        if n_images <= 2:
            cols, rows = n_images, 1
        elif n_images <= 4:
            cols, rows = 2, 2
        elif n_images <= 6:
            cols, rows = 3, 2
        else:
            cols, rows = 3, 3
        
        # è®¡ç®—å¹³å‡å›¾ç‰‡å®½é«˜æ¯”
        avg_ratio = sum(img.width / img.height for img in images) / len(images)
        
        # æ ¹æ®å›¾ç‰‡æ¯”ä¾‹è°ƒæ•´cellå°ºå¯¸
        if avg_ratio > 1.3:  # æ¨ªå‘å›¾ç‰‡
            base_width = min(max_width // cols, 700)
            cell_width = base_width
            cell_height = int(base_width / avg_ratio) + 20
        else:  # æ–¹å½¢æˆ–ç«–å‘å›¾ç‰‡
            base_height = 400
            cell_height = base_height + 20
            cell_width = int(base_height * avg_ratio)
        
        # å›¾ç‰‡resize
        resized_images = []
        for img in images:
            available_width = cell_width - 10
            available_height = cell_height - 35
            ratio = min(available_width / img.width, available_height / img.height)
            new_width = int(img.width * ratio)
            new_height = int(img.height * ratio)
            img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
            resized_images.append(img)
        
        # åˆ›å»ºç½‘æ ¼
        grid_width = cols * cell_width
        grid_height = rows * cell_height
        grid_img = Image.new('RGB', (grid_width, grid_height), color='white')
        
        # æ‹¼æ¥å›¾ç‰‡
        for i, img in enumerate(resized_images):
            if i >= n_images:
                break
                
            row = i // cols
            col = i % cols
            
            cell_x = col * cell_width
            cell_y = row * cell_height
            
            img_x = cell_x + (cell_width - img.width) // 2
            img_y = cell_y + 3
            
            grid_img.paste(img, (img_x, img_y))
        
        return grid_img
    
    def process_single_scene(self, scene_folder):
        """å¤„ç†å•ä¸ªåœºæ™¯ï¼Œè¿”å›æ•°æ®é›†æ¡ç›®"""
        try:
            # åŠ è½½æ•°æ®
            scene_data, image_files = self.load_scene_data(scene_folder)
            
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
            if isinstance(scene_data, list):
                scene_data = scene_data[0]
            
            # ç®€åŒ–å›¾ç»“æ„
            simplified_graph = self.simplify_graph_structure(scene_data)
            
            # æ‹¼æ¥å›¾åƒ
            concat_image = self.concatenate_images(image_files)
            if concat_image is None:
                print(f"âŒ Failed to concatenate images for {scene_folder}")
                return None
            
            # æ„å»ºæ•°æ®é›†æ¡ç›®
            entry = {
                "images": [concat_image],
                "problem": f"<image>Task instruction: {simplified_graph['task_instruction']}",
                "answer": json.dumps(simplified_graph, ensure_ascii=False)
            }
            
            return entry
            
        except Exception as e:
            print(f"âŒ Error processing {scene_folder}: {e}")
            return None
    
    def process_all_scenes(self, base_folders, max_samples=None):
        """å¤„ç†æ‰€æœ‰åœºæ™¯"""
        # æŸ¥æ‰¾æ‰€æœ‰åœºæ™¯
        all_scenes = self.find_all_scenes(base_folders)
        
        if max_samples:
            all_scenes = all_scenes[:max_samples]
            print(f"ğŸ”¬ Processing first {max_samples} scenes for testing")
        
        # å¤„ç†æ¯ä¸ªåœºæ™¯
        processed_data = []
        
        for scene_folder in tqdm(all_scenes, desc="Processing scenes"):
            entry = self.process_single_scene(scene_folder)
            if entry:
                processed_data.append(entry)
        
        print(f"âœ… Successfully processed {len(processed_data)} scenes")
        return processed_data
    
    def create_dataset(self, processed_data, train_ratio=0.95):
        """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
        # éšæœºæ‰“ä¹±
        random.shuffle(processed_data)
        
        # åˆ†å‰²
        total_size = len(processed_data)
        train_size = int(total_size * train_ratio)
        
        train_data = processed_data[:train_size]
        val_data = processed_data[train_size:]
        
        print(f"ğŸ“Š Dataset split: {len(train_data)} train, {len(val_data)} validation")
        
        # åˆ›å»ºDatasetå¯¹è±¡
        train_dataset = Dataset.from_list(train_data)
        val_dataset = Dataset.from_list(val_data)
        
        # åˆ›å»ºDatasetDict
        dataset_dict = DatasetDict({
            "train": train_dataset,
            "validation": val_dataset
        })
        
        return dataset_dict

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    
    # å®šä¹‰æ•°æ®è·¯å¾„
    base_folders = [
        "/code/real",
        "/code/sim_kitchen", 
        "/code/sim_bathroom",
        "/code/sim_bedroom",
        "/code/sim_livingroom"
    ]
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = SceneGraphProcessor()
    
    print("ğŸš€ Scene Graph Dataset Processor")
    print("=" * 50)
    
    # å…ˆæµ‹è¯•10ä¸ªæ ·æœ¬
    print("ğŸ”¬ Testing with 10 samples...")
    processed_data = processor.process_all_scenes(base_folders, max_samples=10)
    
    if processed_data:
        # åˆ›å»ºæ•°æ®é›†
        dataset = processor.create_dataset(processed_data)
        
        # æ‰“å°æ ·æœ¬
        print("\nğŸ“‹ Sample data:")
        print(f"Problem: {dataset['train'][0]['problem']}")
        print(f"Answer: {dataset['train'][0]['answer'][:200]}...")
        
        # ä¿å­˜æµ‹è¯•æ•°æ®é›†
        dataset.save_to_disk("./test_scene_graph_dataset")
        print("ğŸ’¾ Test dataset saved to ./test_scene_graph_dataset")
        
        print("âœ… Test completed! Please verify the data before processing all scenes.")
    else:
        print("âŒ No data processed successfully!")
